{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Analysis using LLMs with Python\n",
    "\n",
    "This notebook demonstrates how to analyze documents using Large Language Models (LLMs). We'll extract text from a PDF document, summarize it, generate questions from the content, and answer those questions using pre-trained models.\n",
    "\n",
    "## Overview\n",
    "\n",
    "Document analysis refers to extracting, interpreting, and understanding the information contained within a document. With the rise of Large Language Models (LLMs) like GPT and BERT, we can now comprehend context, generate summaries, answer questions, and identify key insights efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Libraries\n",
    "\n",
    "First, let's install all the necessary libraries for our document analysis task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install required packages\n",
    "!pip install pdfplumber transformers nltk pandas torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries\n",
    "\n",
    "Now, let's import all the necessary libraries for our document analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Extract Text from PDF\n",
    "\n",
    "For this analysis, we'll use Google's Terms of Service document. First, we need to extract the text from the PDF file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to extract text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "# Path to your PDF file\n",
    "# Note: You need to download the Google Terms of Service PDF\n",
    "# You can replace this with the path to your downloaded PDF\n",
    "pdf_path = \"google_terms_of_service.pdf\"\n",
    "\n",
    "# Check if file exists\n",
    "if os.path.exists(pdf_path):\n",
    "    # Extract text from PDF\n",
    "    extracted_text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # Save extracted text to a file\n",
    "    with open(\"extracted_text.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(extracted_text)\n",
    "    \n",
    "    print(\"Text extracted and saved to extracted_text.txt\")\n",
    "else:\n",
    "    print(f\"File {pdf_path} not found. Please download the Google Terms of Service PDF.\")\n",
    "    # For demonstration purposes, we'll create a sample text\n",
    "    extracted_text = \"\"\"GOOGLE TERMS OF SERVICE\n",
    "Effective May 22, 2024 | Archived versions\n",
    "What's covered in these terms\n",
    "We know it's tempting to skip these Terms of\n",
    "Service, but it's important to establish what you\n",
    "can expect from us as you use Google services,\n",
    "and what we expect from you.\n",
    "These Terms of Service reflect the way Google's business works, the laws that apply to\n",
    "our company, and certain things we've always believed to be true. As a result, these Terms\n",
    "of Service help define Google's relationship with you as you interact with our services. For\n",
    "example, these terms include the following topic headings:\n",
    "What you can expect from us, which describes how we provide and develop our\n",
    "services\n",
    "What we expect from you, which establishes certain rules for using our services\n",
    "Content in Google services, which describes the intellectual property rights to the\n",
    "content you find in our services â€” whether that content belongs to you, Google, or\n",
    "others\n",
    "In case of problems or disagreements, which describes other legal rights you have,\n",
    "and what to expect in case someone violates these terms\n",
    "Understanding these terms is important because, by accessing or using our services,\n",
    "you're agreeing to these terms.\"\"\"\n",
    "    print(\"Using sample text for demonstration purposes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Preview the Extracted Text\n",
    "\n",
    "Let's preview the first few hundred characters of the extracted text to ensure everything is correctly captured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Preview the first 1000 characters of the extracted text\n",
    "print(extracted_text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Summarize the Document\n",
    "\n",
    "Now, let's use a pre-trained summarization model to get a high-level overview of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize the summarization pipeline with t5-small model\n",
    "summarizer = pipeline(\"summarization\", model=\"t5-small\")\n",
    "\n",
    "# Summarize the first 1000 characters of the document\n",
    "# Note: T5 has a limit on input length, so we're only summarizing a portion\n",
    "document_summary = summarizer(extracted_text[:1000], max_length=150, min_length=30, do_sample=False)\n",
    "\n",
    "# Print the summary\n",
    "print(\"Summary:\", document_summary[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Split the Document into Sentences and Passages\n",
    "\n",
    "For more detailed analysis, we need to split the document into smaller chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Tokenize the document into sentences\n",
    "sentences = sent_tokenize(extracted_text)\n",
    "\n",
    "# Combine sentences into passages (chunks of text)\n",
    "passages = []\n",
    "current_passage = \"\"\n",
    "word_limit = 200  # Limit each passage to approximately 200 words\n",
    "\n",
    "for sentence in sentences:\n",
    "    # Add the sentence to the current passage\n",
    "    temp_passage = current_passage + \" \" + sentence if current_passage else sentence\n",
    "    \n",
    "    # Check if adding this sentence exceeds the word limit\n",
    "    if len(temp_passage.split()) > word_limit and current_passage:\n",
    "        # If it does, add the current passage to the list and start a new one\n",
    "        passages.append(current_passage)\n",
    "        current_passage = sentence\n",
    "    else:\n",
    "        # If not, update the current passage\n",
    "        current_passage = temp_passage\n",
    "\n",
    "# Add the last passage if it's not empty\n",
    "if current_passage:\n",
    "    passages.append(current_passage)\n",
    "\n",
    "# Print the number of passages created\n",
    "print(f\"Document split into {len(passages)} passages.\")\n",
    "\n",
    "# Preview the first passage\n",
    "print(\"\\nPassage 1:\\n\", passages[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Generate Questions from the Passages\n",
    "\n",
    "Now, let's generate questions based on the document's content using a question generation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize the question generation pipeline\n",
    "question_generator = pipeline(\"text2text-generation\", model=\"valhalla/t5-base-qg-hl\")\n",
    "\n",
    "def generate_questions_pipeline(text, min_questions=3):\n",
    "    # Generate questions from the text\n",
    "    questions = question_generator(f\"generate questions: {text}\", max_length=512, num_return_sequences=min_questions)\n",
    "    return [q['generated_text'] for q in questions]\n",
    "\n",
    "# Generate questions for each passage\n",
    "all_questions = []\n",
    "for i, passage in enumerate(passages[:5]):  # Limit to first 5 passages for demonstration\n",
    "    print(f\"Passage {i+1}:\\n{passage[:500]}...\\n\")  # Print first 500 chars of passage\n",
    "    \n",
    "    # Generate at least 3 questions for each passage\n",
    "    questions = generate_questions_pipeline(passage)\n",
    "    \n",
    "    # If we couldn't generate enough questions, try with smaller chunks\n",
    "    if len(questions) < 3 and len(passage.split()) > 100:\n",
    "        # Split the passage into smaller chunks\n",
    "        sentences = sent_tokenize(passage)\n",
    "        mid = len(sentences) // 2\n",
    "        chunk1 = \" \".join(sentences[:mid])\n",
    "        chunk2 = \" \".join(sentences[mid:])\n",
    "        \n",
    "        # Generate questions from each chunk\n",
    "        questions1 = generate_questions_pipeline(chunk1)\n",
    "        questions2 = generate_questions_pipeline(chunk2)\n",
    "        questions = questions1 + questions2\n",
    "    \n",
    "    print(\"Generated Questions:\")\n",
    "    for q in questions:\n",
    "        print(f\"- {q}\")\n",
    "        all_questions.append((q, passage))  # Store question with its context\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Answer the Generated Questions\n",
    "\n",
    "Finally, let's use a question-answering model to find answers to the generated questions within the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize the question-answering pipeline\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\")\n",
    "\n",
    "# Function to answer unique questions\n",
    "def answer_unique_questions(questions_with_context):\n",
    "    answered_questions = set()  # To track which questions we've already answered\n",
    "    \n",
    "    for question, context in questions_with_context:\n",
    "        # Skip if we've already answered this question\n",
    "        if question in answered_questions:\n",
    "            continue\n",
    "        \n",
    "        # Add to the set of answered questions\n",
    "        answered_questions.add(question)\n",
    "        \n",
    "        # Get the answer\n",
    "        answer = qa_pipeline(question=question, context=context)\n",
    "        \n",
    "        # Print the question and answer\n",
    "        print(f\"Q: {question}\")\n",
    "        print(f\"A: {answer['answer']}\\n\")\n",
    "\n",
    "# Answer the generated questions\n",
    "answer_unique_questions(all_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how to use Large Language Models (LLMs) for document analysis. We've:\n",
    "\n",
    "1. Extracted text from a PDF document\n",
    "2. Summarized the document using a pre-trained model\n",
    "3. Split the document into manageable passages\n",
    "4. Generated questions from the passages\n",
    "5. Answered those questions using a question-answering model\n",
    "\n",
    "This approach can be applied to various types of documents for information extraction, summarization, and question answering tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
